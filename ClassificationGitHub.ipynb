{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classification code designed to sample and separate training and test data. \n",
    "Scaling workflow is included within this code with the scale applied to high spatial \n",
    "resolution data. Any tuning or training is done using the variables created in this cell. \n",
    "\"\"\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn import metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import statistics as st\n",
    "from itertools import combinations \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True, \n",
    "                         save_path = \"\"):\n",
    "    \n",
    "    accuracy = (np.trace(cm) / float(np.sum(cm))) * 100\n",
    "    titlefont = {\"fontname\":\"Times New Roman\", \"fontsize\" : 35, \"fontweight\": \"bold\"}\n",
    "    labelfont = {\"fontname\":\"Times New Roman\", \"fontsize\" : 25}\n",
    "    misclass = (1 - (accuracy/100)) * 100\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap) \n",
    "    plt.grid(False)\n",
    "    cb = plt.colorbar(fraction = 0.046, pad = 0.04)\n",
    "    cb.ax.tick_params(labelsize='large')\n",
    "    pattern = \"[A-Z][^A-Z]*\"\n",
    "    titlelist = re.findall(pattern, title)\n",
    "    print(mpl.rcParams[\"savefig.dpi\"])\n",
    "    if \"MLPClassifier\" in title:\n",
    "        newtitle = \"\".join(titlelist[:3]) + \" \" + titlelist[3]\n",
    "    elif title == \"LinearSVC\":\n",
    "        newtitle = \"\".join(titlelist[-3:-1]) + \"M Classifier\"\n",
    "    else:\n",
    "        newtitle = \" \".join(titlelist)\n",
    "    plt.title(newtitle, fontdict = titlefont)\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation = 45, **labelfont)\n",
    "        plt.yticks(tick_marks, target_names, rotation = 0, **labelfont)\n",
    "\n",
    "    if normalize:\n",
    "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]) * 100\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color = \"white\" if cm[i, j] > thresh else \"black\", **labelfont)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color = \"white\" if cm[i, j] > thresh else \"black\", **labelfont)\n",
    "            \n",
    "    if save_path == \"\":\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel(\"True label\", fontdict = labelfont)\n",
    "        plt.xlabel(\n",
    "            \"Predicted label\\naccuracy={:0.2f}; misclass={:0.2f}\".format(\n",
    "                accuracy,misclass), fontdict = labelfont\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        plt.ylabel(\"True label\", labelfont)\n",
    "        plt.xlabel(\n",
    "            \"Predicted label\\naccuracy={:0.2f}; misclass={:0.2f}\".format(\n",
    "                accuracy, misclass), fontdict = labelfont\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi = 300)\n",
    "        plt.show()\n",
    "\n",
    "iteration = 16\n",
    "\n",
    "classes_path_root = r\"D:/Documents/PhD/GIS/Classification_Code_Output/FHkHz\"\n",
    "\n",
    "classes_path_base = r\"/Final_Classification\" + str(iteration)\n",
    "\n",
    "wex_path_root = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Wex\"\n",
    "\n",
    "wex_path_base = r\"/Final_Classification\" + str(iteration)\n",
    "\n",
    "hem_path_root = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Hemp\"\n",
    "\n",
    "hem_path_base = r\"/Final_Classification\" + str(iteration)\n",
    "\n",
    "csv_path = (\n",
    "    r\"D:/Documents/PhD/GIS/Classification_Code_Output/FHkHz/Samples BPI2 Copyrev.csv\"\n",
    ")\n",
    "\n",
    "classes_path_new = (\n",
    "    r\"D:/Documents/PhD/GIS/Classification_Code_Output/FHkHz/Final_Classification\" \\\n",
    "    + str(iteration) + \".shp\"\n",
    ")\n",
    "\n",
    "cv_path_root = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Cross_Validation\"\n",
    "\n",
    "classreport_path_root = (\n",
    "    r\"D:/Documents/PhD/GIS/Classification_Code_Output/Classification_Reports_Confusion_Matrices\"\n",
    ")\n",
    "\n",
    "corkcm_image = classes_path_root + classes_path_base\n",
    "\n",
    "wexs_classes_path_new = wex_path_root + wex_path_base + \"s.shp\"\n",
    "\n",
    "wexn_classes_path_new = wex_path_root + wex_path_base + \"n.shp\"\n",
    "\n",
    "wextotal_classes_path_new = wex_path_root + wex_path_base + \"to.shp\"\n",
    "\n",
    "hem_classes_path_new = hem_path_root + hem_path_base + \".shp\"\n",
    "\n",
    "csv_file = pd.read_csv(csv_path, delimiter = \";\", header = 0)\n",
    "\n",
    "wexscm_image = wex_path_root + wex_path_base + \"s\"\n",
    "\n",
    "wexncm_image = wex_path_root + wex_path_base + \"n\"\n",
    "\n",
    "wextocm_image = wex_path_root + wex_path_base + \"to\"\n",
    "\n",
    "hemcm_image = hem_path_root + hem_path_base \n",
    "\n",
    "wexhemcm_image = wex_path_root + \"Hem\" + \"/\" + wex_path_base\n",
    "\n",
    "intersect_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/FHkHz/BPIInterJoin2vii.shp\"\n",
    "\n",
    "hemshape_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Hemp/HemJoin5revii.shp\"\n",
    "\n",
    "wexsshape_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Wex/WexSJoin5rev.shp\"\n",
    "\n",
    "wexseshape_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Wex/WexSeJoin5rev.shp\"\n",
    "\n",
    "wexswshape_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Wex/WexSwJoin5rev.shp\"\n",
    "\n",
    "wexnshape_path = r\"D:/Documents/PhD/GIS/Classification_Code_Output/Wex/WexNJoin5rev.shp\"\n",
    "\n",
    "wex_classes_path = (\n",
    "    r\"D:/Documents/PhD/GIS/Classification_Code_Output/WexHem/SamplesWexHemviirevcorvi.csv\"\n",
    ")\n",
    "\n",
    "col_drop_normalisation = [\n",
    "    \"geometry\", \"Class\", \"Flat\", \"Ridge\", \"Valley\", \"RelBorderR\", \"RelBorderV\",\n",
    "    \"HueSlopeCos\", \"HueSlopeSin\",\n",
    "]\n",
    "\n",
    "rename_dict = {\n",
    "    \"Classifi_1\": \"Ridge\", \"Classified\": \"Flat\", \"Classifi_2\": \"Valley\", \"Mean_BPI25\": \"bpi25z\", \n",
    "    \"Mean_BPI9\": \"bpi9z\", \"HSI_Transf\": \"HueSlopeSin\", \"HSI_Tran_1\": \"HueSlopeCos\", \n",
    "    \"VRM25z\": \"vrm25z\", \"Zero9\": \"zero9z\", \"Zero3\": \"Mean_Zerom\",\"VRM9z\": \"vrm9z\", \n",
    "    \"Rel_border\": \"RelBorderV\", \"Rel_bord_1\": \"RelBorderR\"\n",
    "}\n",
    "\n",
    "extra_dict = {\n",
    "    \"HSV_HueR_1\": \"HueSlopeSin\", \"HSV_HueRSl\": \"HueSlopeCos\", \n",
    "    \"Border_Len\" : \"Border_L_1\", \"Mean_Zero9\": \"zero9z\", \"Mean_BPI3\": \"Mean_BPI\",\n",
    "     \"Mean_VRM3\": \"Mean_VRM\"\n",
    "}\n",
    "\n",
    "neulabel = \"NeuClass\"\n",
    "\n",
    "neulabel2 = \"NeuClass2\"\n",
    "\n",
    "svmlabel = \"SVM_Class\"\n",
    "\n",
    "knnlabel = \"KNN_Class\"\n",
    "\n",
    "rfclabel = \"Random_Forest_Class\"\n",
    "\n",
    "voting = \"Vote_Class\"\n",
    "\n",
    "def dataset_split(input_classes, input_objects, rename_dict, valid_or_train, n_classes):\n",
    "    \n",
    "    csv_file = pd.read_csv(input_classes, delimiter = \";\", header = 0)\n",
    "    \n",
    "    class_id_list = csv_file[\"id\"].to_list()\n",
    "    \n",
    "    class_names_list = csv_file[\"Class_Names\"].to_list()\n",
    "    \n",
    "    print(np.unique(class_names_list))\n",
    "    \n",
    "    csv_dictionary = dict(zip(class_id_list, class_names_list))\n",
    "    \n",
    "    csv_classes = list(csv_dictionary.values())\n",
    "    \n",
    "    objects_total = gpd.read_file(input_objects)  \n",
    "    \n",
    "    objects_total[\"Class\"] = np.select(\n",
    "        [objects_total[\"Class\"] == x for x in csv_dictionary], csv_dictionary.values(), \"Flat\"\n",
    "    )\n",
    "    \n",
    "    columns_to_remove = [\n",
    "        val for val in objects_total.columns if \"modeMed\" in val\n",
    "    ]\n",
    "    \n",
    "    objects_total = objects_total.replace(-9999, np.nan)\n",
    "    \n",
    "    objects_total = objects_total.rename(columns = rename_dict)\n",
    "    \n",
    "    waves = objects_total.loc[\n",
    "        ((objects_total[\"Valley\"] == 1) | (objects_total[\"Ridge\"] == 1)) |\\\n",
    "        (objects_total[\"Class\"].isin(csv_classes))\n",
    "    ]\n",
    "    \n",
    "    lower_list = [val for val in csv_classes if \"Lower\" in val]\n",
    "    \n",
    "    higher_list = [val for val in csv_classes if \"Upper\" in val]\n",
    "    \n",
    "    not_waves = objects_total.loc[objects_total[\"Flat\"] == 1]\n",
    "    \n",
    "    samples = waves.loc[waves[\"Class\"].isin(csv_classes)]\n",
    "    \n",
    "    upper = samples.loc[samples[\"Class\"].isin(higher_list)]\n",
    "\n",
    "    lower = samples.loc[samples[\"Class\"].isin(lower_list)]\n",
    "    \n",
    "    if valid_or_train == \"train\": \n",
    "        correct_lower = lower.loc[(lower[\"Valley\"] == 1) | (lower[\"Flat\"] == 1) \n",
    "        ] # the or flat here preserves the original samples\n",
    "        correct_upper = upper.loc[(upper[\"Ridge\"] == 1) | (upper[\"Flat\"] == 1)\n",
    "        ] # the same as above\n",
    "        samples = pd.concat([correct_lower, correct_upper])\n",
    "    else: \n",
    "        correct_lower = lower.loc[lower[\"Valley\"] == 1] #All flat samples are excluded\n",
    "        correct_upper = upper.loc[upper[\"Ridge\"] == 1] #All flat samples are excluded\n",
    "        samples = pd.concat([correct_lower, correct_upper])\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        samples[\"Class\"] = samples.apply(\n",
    "            lambda row: class_split(row, \"Class\"), axis = 1\n",
    "        )\n",
    "    \n",
    "    samples = pd.DataFrame(samples)\n",
    "    \n",
    "    samples = samples.copy()\n",
    "    \n",
    "    samples_group = samples.groupby(\"Class\", as_index = False)\n",
    "\n",
    "    samples_equal = samples_group.apply(\n",
    "        lambda x: x.sample(samples_group.size().min(), \n",
    "                           random_state = 42).reset_index(drop = False)\n",
    "    )\n",
    "    \n",
    "    samples_equal_df = samples.loc[samples_equal[\"index\"].values, :]\n",
    "    \n",
    "    test_data = waves.loc[waves.index.difference(samples_equal_df.index)]\n",
    "    \n",
    "    if (not_waves[not_waves.columns.difference([\"Class\"])].empty) and (test_data.empty):\n",
    "        print(\"\\nSamples Equal only\")\n",
    "        return samples_equal_df, csv_dictionary\n",
    "    elif samples.empty:\n",
    "        print(\"\\nTest and Waves only\")\n",
    "        not_waves[\"Class\"] = \"Flat\"\n",
    "        return test_data, not_waves\n",
    "    elif not_waves[not_waves.columns.difference([\"Class\"])].empty:\n",
    "        print(\"\\nSamples and Test only\")\n",
    "        return samples_equal_df, test_data\n",
    "    else:\n",
    "        print(\"\\nAll Data\")\n",
    "        return samples_equal_df, test_data, not_waves, csv_dictionary\n",
    "\n",
    "interpol = \"linear\"\n",
    "\n",
    "def prep_for_norm(\n",
    "    samples = pd.DataFrame(), test_data = pd.DataFrame(), not_waves_data = pd.DataFrame(), \n",
    "    columns_to_ignore = [], valid_or_train = None\n",
    "):\n",
    "    \n",
    "    sample_classes = samples.Class.to_numpy()\n",
    "    \n",
    "    cols = columns_to_ignore\n",
    "    \n",
    "    sample_variables = samples.drop(\n",
    "        columns = cols, errors = \"ignore\"\n",
    "    )\n",
    "    \n",
    "    sample_variables = sample_variables.sort_index(axis = 1)\n",
    "    \n",
    "    print(sample_variables.columns)\n",
    "    \n",
    "    sample_variables[\"Class\"] = sample_classes \n",
    "\n",
    "    sample_variables_df = pd.DataFrame(sample_variables)\n",
    "    \n",
    "    if not sample_variables_df.empty:\n",
    "        if sample_variables_df.isnull().values.any() == True:\n",
    "            sample_variables_df.interpolate(\n",
    "                method = interpol, order = 2, axis = 0, inplace = True, \n",
    "                limit_direction = \"both\"\n",
    "            )\n",
    "        \n",
    "        if sample_variables_df.isnull().values.any() == True:\n",
    "            sample_variables_df.interpolate(\n",
    "                method = \"linear\", order = 2, inplace = True,\n",
    "                limit_direction = \"both\"\n",
    "            )\n",
    "    sample_classes = sample_variables_df.Class.to_numpy()\n",
    "    \n",
    "    print(np.unique(sample_classes))\n",
    "    \n",
    "    sample_variables_df = sample_variables_df.drop(columns = [\"Class\"])\n",
    "    \n",
    "    sample_var_array = sample_variables_df.to_numpy()\n",
    "\n",
    "    collist = sample_variables_df.columns\n",
    "       \n",
    "    test_data_variables = test_data.drop(columns = cols, errors = \"ignore\")\n",
    "\n",
    "    test_data_variables = test_data_variables.sort_index(axis = 1)\n",
    "\n",
    "    test_data_variables_df = pd.DataFrame(test_data_variables)\n",
    "    \n",
    "    if not test_data_variables_df.empty:\n",
    "        if test_data_variables_df.isnull().values.any() == True:\n",
    "            print(test_data_variables_df.isnull().values.any())\n",
    "            test_data_variables_df.interpolate(\n",
    "                method = interpol, order = 2, axis = 0, inplace = True, \n",
    "                limit_direction = \"both\"\n",
    "            )\n",
    "        \n",
    "        if test_data_variables_df.isnull().values.any() == True:\n",
    "            test_data_variables_df.interpolate(\n",
    "                method = \"linear\", order = 2, inplace = True,\n",
    "                limit_direction = \"both\"\n",
    "            )\n",
    "        \n",
    "    test_data_array = test_data_variables_df.to_numpy()\n",
    "    \n",
    "    if samples.empty:\n",
    "        if not_waves_variables.isnull().values.any() == True:\n",
    "            not_waves_variables = not_waves_data.drop(columns = cols, errors = \"ignore\")\n",
    "\n",
    "            not_waves_variables = not_waves_variables.sort_index(axis = 1)\n",
    "\n",
    "            not_waves_variables = pd.DataFrame(not_waves_variables)\n",
    "\n",
    "            not_waves_variables.interpolate(\n",
    "                method = interpol, order = 2, axis = 0, inplace = True, \n",
    "                limit_direction = \"both\")\n",
    "        \n",
    "        if not_waves_variables.isnull().values.any() == True:\n",
    "            not_waves_variables.interpolate(\n",
    "                method = \"linear\", order = 2, inplace = True,\n",
    "                limit_direction = \"both\"\n",
    "            )\n",
    "        \n",
    "        not_waves_array = not_waves_variables.to_numpy()\n",
    "        \n",
    "        return test_data_array, not_waves_array\n",
    "    \n",
    "    elif (test_data.empty) and (not_waves_data.empty):\n",
    "                \n",
    "        if valid_or_train == \"train\":\n",
    "            \n",
    "            return sample_classes, sample_var_array, collist\n",
    "        else:\n",
    "            \n",
    "            return sample_classes, sample_var_array\n",
    "    \n",
    "    elif not_waves_data.empty:\n",
    "        \n",
    "        if valid_or_train == \"train\":\n",
    "            \n",
    "            return sample_classes, sample_var_array, test_data_array, collist\n",
    "        else:\n",
    "            \n",
    "            return sample_classes, sample_var_array, test_data_array \n",
    "    else:         \n",
    "        \n",
    "        not_waves_variables = not_waves_data.drop(columns = cols, errors = \"ignore\")\n",
    "\n",
    "        not_waves_variables = not_waves_variables.sort_index(axis = 1)\n",
    "\n",
    "        not_waves_variables = pd.DataFrame(not_waves_variables)\n",
    "        if not_waves_variables.isnull().values.any() == True:\n",
    "            not_waves_variables.interpolate(method = interpol, order = 2,\n",
    "                                            axis = 0, inplace = True, limit_direction = \"both\")\n",
    "        \n",
    "        if not_waves_variables.isnull().values.any() == True:\n",
    "            not_waves_variables.interpolate(method = \"linear\", order = 2, inplace = True,\n",
    "                                           limit_direction = \"both\")\n",
    "        \n",
    "        not_waves_variables\n",
    "\n",
    "        not_waves_array = not_waves_variables.to_numpy()\n",
    "        \n",
    "        if valid_or_train == \"train\":\n",
    "            print(\"\\nAll data with column list\")\n",
    "            return sample_classes, sample_var_array, test_data_array, not_waves_array, collist\n",
    "        else:\n",
    "            print(\"\\nAll data without column list\")\n",
    "            return sample_classes, sample_var_array, test_data_array, not_waves_array\n",
    "\n",
    "def scaled_data_train(sample_var_array, test_data_array, not_waves_array, scaler):\n",
    "    if scaler == \"minmax\":\n",
    "        print(\"\\nMinmax\")\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(sample_var_array)\n",
    "        sample_stan = scaler.transform(sample_var_array)\n",
    "        test_stan = scaler.transform(test_data_array)\n",
    "        not_waves_stan = scaler.transform(not_waves_array)\n",
    "    elif scaler == \"standard\":\n",
    "        print(\"\\nStandard\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(sample_var_array)\n",
    "        sample_stan = scaler.transform(sample_var_array)\n",
    "        test_stan = scaler.transform(test_data_array)\n",
    "        not_waves_stan = scaler.transform(not_waves_array)\n",
    "    return sample_stan, test_stan, not_waves_stan, scaler\n",
    "\n",
    "def scaled_valid_deploy(\n",
    "    scaler, sample_var_array = np.array([]), test_data_array = np.array([]),\n",
    "    not_waves_array = np.array([])\n",
    "):\n",
    "    valid_stan = scaler.transform(sample_var_array)\n",
    "    \n",
    "    if (test_data_array.size == 0) and (not_waves_array.size == 0):\n",
    "        print(\"\\nValid standardised\")\n",
    "        return valid_stan \n",
    "    \n",
    "    elif not_waves_array.size == 0:\n",
    "        \n",
    "        test_stan = scaler.transform(test_data_array) \n",
    "        print(\"\\nValid and test standardised\")\n",
    "        return valid_stan, test_stan\n",
    "    else:\n",
    "        test_stan = scaler.transform(test_data_array) \n",
    "        \n",
    "        not_waves_stan = scaler.transform(not_waves_array)\n",
    "        #print(\"\\nAll Data Standardised\")\n",
    "        return valid_stan, test_stan, not_waves_stan\n",
    "\n",
    "def model_deploy(model, train_data = np.array([]), test_data = np.array([])):\n",
    "    \n",
    "    train_pred = model.predict(train_data)\n",
    "    \n",
    "    if test_data.size == 0:\n",
    "        return train_pred\n",
    "    else:\n",
    "        test_pred = model.predict(test_data)\n",
    "        return train_pred, test_pred\n",
    "\n",
    "def create_df(original_data, data, model_pred_dict, header, n_classes, index = []):\n",
    "    \n",
    "    if len(index) == 0:\n",
    "        data_df = pd.DataFrame(data, columns = header)   \n",
    "        \n",
    "    else:\n",
    "        data_df = pd.DataFrame(data, columns = header, index = index)\n",
    "        \n",
    "    ridge = original_data[\"Ridge\"].to_numpy()\n",
    "\n",
    "    valley = original_data[\"Valley\"].to_numpy()\n",
    "\n",
    "    flat = original_data[\"Flat\"].to_numpy()\n",
    "    \n",
    "    geometry = original_data[\"geometry\"].to_numpy()\n",
    "    \n",
    "    labels = original_data[\"Class\"].to_numpy()\n",
    "\n",
    "    data_df[\"Ridge\"] = ridge\n",
    "\n",
    "    data_df[\"Valley\"] = valley\n",
    "\n",
    "    data_df[\"Flat\"] = flat\n",
    "\n",
    "    data_df[\"geometry\"] = geometry\n",
    "    \n",
    "    data_df[\"Labels\"] = labels\n",
    "\n",
    "    if n_classes == 2:\n",
    "        for model, classes in model_pred_dict.items():\n",
    "            data_df[model] = classes       \n",
    "            data_df[model] = data_df.apply(\n",
    "                lambda row: class_reunite(row, model), axis = 1\n",
    "            )\n",
    "    else:\n",
    "        for model, classes in model_pred_dict.items():\n",
    "            data_df[model] = classes\n",
    "    return data_df\n",
    "\n",
    "def create_id_file(path1, path2, sampleshape, csv_path, csv_dict, test_data, model1, model2):\n",
    "    idfilename = path1.split(\".\")[0] + \".txt\"\n",
    "    idfile = open(idfilename, \"w+\")\n",
    "    idfile.write(\n",
    "        \"Shapefile used:\" + os.path.basename(sampleshape)\n",
    "    )\n",
    "    idfile.write(\"\\nSample file used: \" + os.path.basename(csv_path).split(\".\")[0] + \".asc\")\n",
    "    idfile.write(\"\\nSamples used: \" + str(csv_dict))\n",
    "    idfile.write(\"\\nNumber of variables used: \" + str(len(\n",
    "        [x for x in test_data.columns if x not in col_drop_normalisation]\n",
    "        )))\n",
    "    idfile.write(\"\\nVariables used: \" + str(\n",
    "        [x for x in test_data.columns if x not in col_drop_normalisation]\n",
    "    ))\n",
    "    idfile.write(\"\\nHyperparameters chosen for first neunet: \" + str(model1.get_params))\n",
    "    idfile.write(\"\\nHyperparameters chosen for second neunet: \" + str(model2.get_params))\n",
    "    idfile.close()\n",
    "    return\n",
    "\n",
    "def accuracy_analysis(train_classes, predicted_classes, label, class_label, save_path = \"\"):\n",
    "    \n",
    "    cm = confusion_matrix(train_classes, predicted_classes, labels = class_label)\n",
    "    \n",
    "    if save_path == \"\":\n",
    "        plot_confusion_matrix(\n",
    "            cm, class_label, title = label, cmap = plt.cm.Blues, normalize = True,\n",
    "            save_path = \"\")            \n",
    "\n",
    "    else:\n",
    "        image_path = save_path + label + \".png\"\n",
    "\n",
    "        report_csv_path = save_path + label + \"report.csv\"\n",
    "\n",
    "        plot_confusion_matrix(\n",
    "            cm, class_label, title = label, cmap = plt.cm.Blues, normalize = True,\n",
    "            save_path = image_path\n",
    "        )\n",
    "\n",
    "        report = metrics.classification_report(\n",
    "            train_classes, predicted_classes, output_dict = True\n",
    "        )\n",
    "        \n",
    "        report[\"Kappa_Coefficient\"] = metrics.cohen_kappa_score(\n",
    "            train_classes, predicted_classes\n",
    "        )\n",
    "\n",
    "        report_df = pd.DataFrame.from_dict(report)\n",
    "\n",
    "        report_df.to_csv(report_csv_path)\n",
    "    \n",
    "    print(metrics.classification_report(train_classes, predicted_classes))\n",
    "    kappa_score = round(metrics.cohen_kappa_score(train_classes, predicted_classes), 3)\n",
    "    print(\"\\nKappa Score: \", kappa_score)\n",
    "    accuracy_score = round(\n",
    "        metrics.balanced_accuracy_score(train_classes, predicted_classes), 3)\n",
    "    print(\"Accuracy Score: \", accuracy_score)\n",
    "    macro_precision = round(metrics.precision_score(train_classes, predicted_classes, \n",
    "                                                   average = \"macro\"), 3)\n",
    "    print(\"\\nMacro Precision: \", macro_precision)\n",
    "    micro_precision = round(metrics.precision_score(train_classes, predicted_classes,\n",
    "                                                    average = \"micro\"), 3)\n",
    "    print(\"Micro Precision: \", micro_precision)\n",
    "    macro_recall = round(\n",
    "        metrics.recall_score(train_classes, predicted_classes, average = \"macro\"), 3)\n",
    "    print(\"\\nMacro Recall: \", macro_recall)\n",
    "    micro_recall = round(\n",
    "        metrics.recall_score(train_classes, predicted_classes, average = \"micro\"), 3)\n",
    "    print(\"Micro Recall: \", micro_recall)\n",
    "    macro_f1 = round(\n",
    "        metrics.f1_score(train_classes, predicted_classes, average = \"macro\"), 3)\n",
    "    print(\"Macro F1: \", macro_f1)\n",
    "    \n",
    "train = \"train\"\n",
    "valid = \"valid\"\n",
    "scale_type = \"standard\"\n",
    "n_classes = 4\n",
    "\n",
    "cork_train_model_dict = {}\n",
    "\n",
    "cork_test_model_dict = {}\n",
    "cork_not_waves_dict = {}\n",
    "\n",
    "wexto_train_model_dict = {}\n",
    "wexto_test_model_dict = {}\n",
    "wexto_not_waves_dict = {}\n",
    "\n",
    "wexse_train_model_dict = {}\n",
    "wexse_test_model_dict = {}\n",
    "wexse_not_waves_dict = {}\n",
    "\n",
    "wexsw_train_model_dict = {}\n",
    "wexsw_test_model_dict = {}\n",
    "wexsw_not_waves_dict = {}\n",
    "\n",
    "wexs_train_model_dict = {}\n",
    "wexs_test_model_dict = {}\n",
    "wexs_not_waves_dict = {}\n",
    "\n",
    "wexn_train_model_dict = {}\n",
    "wexn_test_model_dict = {}\n",
    "wexn_not_waves_dict = {}\n",
    "\n",
    "hem_train_model_dict = {}\n",
    "hem_test_model_dict = {}\n",
    "hem_not_waves_dict = {}\n",
    "\n",
    "print(\"\\nCork Harbour\\n\")\n",
    "\n",
    "samples_equal, test_data, not_waves, cork_dict = dataset_split(\n",
    "    input_classes = csv_path,  input_objects = intersect_path, rename_dict = rename_dict,\n",
    "    valid_or_train = valid, n_classes = n_classes\n",
    ")\n",
    "\n",
    "sample_classes, sample_var_array, test_array, not_waves_array, collist = prep_for_norm(\n",
    "    samples = samples_equal, test_data = test_data, not_waves_data = not_waves, \n",
    "    columns_to_ignore = col_drop_normalisation, valid_or_train = train\n",
    ")\n",
    "\n",
    "sample_scaled, test_scaled, not_waves_scaled, scaler = scaled_data_train(\n",
    "     sample_var_array, test_array, not_waves_array, scale_type\n",
    ")\n",
    "\n",
    "mlp1_params = {\n",
    "    'alpha': 0.001, 'hidden_layer_sizes': [22, 20, 16, 8], \n",
    "    'learning_rate_init': 0.01, 'random_state': 396\n",
    "}\n",
    "\n",
    "mlp2_params = {\n",
    "    'alpha': 0.1, 'hidden_layer_sizes': [20, 22, 22, 20], \n",
    "    'learning_rate_init': 0.001, 'random_state': 173\n",
    "}\n",
    "\n",
    "neunet_model = MLPClassifier(**mlp1_params)\n",
    "\n",
    "neunet_model2 = MLPClassifier(**mlp2_params)\n",
    "\n",
    "estimators = [\n",
    "    (\"mlp1\", MLPClassifier(**mlp1_params)),\n",
    "    (\"mlp2\", MLPClassifier(**mlp2_params))\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "weights = [1, 1]\n",
    "\n",
    "clf = VotingClassifier(estimators, voting = \"soft\", weights = weights)\n",
    "\n",
    "clf.fit(sample_scaled, sample_classes)\n",
    "\n",
    "cork_train_model_dict[voting], cork_test_model_dict[voting] = model_deploy(\n",
    "    clf, sample_scaled, test_scaled\n",
    ")\n",
    "\n",
    "cork_not_waves_dict[voting] = \"Flat\"\n",
    "\n",
    "neunet_model.fit(sample_scaled, sample_classes)\n",
    "\n",
    "neunet_model2.fit(sample_scaled, sample_classes)\n",
    "\n",
    "cork_train_model_dict[neulabel], cork_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, sample_scaled, test_scaled\n",
    ") #reminder that the outputs from model deploy are the train pred and test pred\n",
    "\n",
    "cork_train_model_dict[neulabel2], cork_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, sample_scaled, test_scaled\n",
    ")\n",
    "\n",
    "cork_not_waves_dict[neulabel] = \"Flat\"\n",
    "\n",
    "cork_not_waves_dict[neulabel2] = \"Flat\"\n",
    "\n",
    "svm_params = {\n",
    "    'C': 100, 'dual': False, 'fit_intercept': True,\n",
    "    'loss': 'squared_hinge', 'penalty': 'l1', \n",
    "    'random_state': 57, 'tol': 0.1\n",
    "}\n",
    "\n",
    "svm_model = LinearSVC(**svm_params)\n",
    "\n",
    "svm_model.fit(sample_scaled, sample_classes)\n",
    "\n",
    "cork_train_model_dict[svmlabel], cork_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, sample_scaled, test_scaled\n",
    ")\n",
    "\n",
    "cork_not_waves_dict[svmlabel] = \"Flat\"\n",
    "\n",
    "print(\"\\nAll Models Trained!!!\")\n",
    "\n",
    "print(\"\\nHemptons\\n\")\n",
    "cork_not_waves_dict[rfclabel] = \"Flat\"\n",
    "\n",
    "cork_not_waves_dict[knnlabel] = \"Flat\"\n",
    "\n",
    "rename_dict.update(extra_dict) # to ensure that all datasets have the same column\n",
    "\n",
    "hem_samples_equal, hem_test_data, hem_not_waves, hem_dict = dataset_split(\n",
    "    wex_classes_path, hemshape_path, rename_dict, valid, n_classes\n",
    ")\n",
    "\n",
    "hem_sample_classes, hem_sample_array, hem_test_array, hem_nwaves_array =  prep_for_norm(\n",
    "    hem_samples_equal, hem_test_data, hem_not_waves, col_drop_normalisation\n",
    ")\n",
    "\n",
    "hem_sample_scaled, hem_test_scaled, hem_nwaves_scaled = scaled_valid_deploy(\n",
    "     scaler, hem_sample_array, hem_test_array, hem_nwaves_array\n",
    " )\n",
    "\n",
    "hem_train_model_dict[neulabel], hem_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, hem_sample_scaled, hem_test_scaled\n",
    ")\n",
    "\n",
    "hem_train_model_dict[neulabel2], hem_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, hem_sample_scaled, hem_test_scaled\n",
    ")\n",
    "\n",
    "hem_train_model_dict[voting], hem_test_model_dict[voting] = model_deploy(\n",
    "    clf, hem_sample_scaled, hem_test_scaled\n",
    ")\n",
    "\n",
    "hem_train_model_dict[svmlabel], hem_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, hem_sample_scaled, hem_test_scaled\n",
    ")\n",
    "\n",
    "wexs_samples_equal, wexs_test_data, wexs_not_waves, wexs_dict = dataset_split(\n",
    "    wex_classes_path, wexsshape_path, rename_dict, valid, n_classes\n",
    ")\n",
    "\n",
    "\n",
    "wexse_samples_equal, wexse_test_data, wexse_not_waves, wexse_dict = dataset_split(\n",
    "    wex_classes_path, wexseshape_path, rename_dict, valid, n_classes\n",
    ")\n",
    "\n",
    "wexsw_samples_equal, wexsw_test_data, wexsw_not_waves, wexsw_dict = dataset_split(\n",
    "    wex_classes_path, wexswshape_path, rename_dict, valid, n_classes\n",
    ")\n",
    "\n",
    "wexn_samples_equal, wexn_test_data, wexn_not_waves, wexn_dict = dataset_split(\n",
    "    wex_classes_path, wexnshape_path, rename_dict, valid, n_classes\n",
    ")\n",
    "\n",
    "print(\"\\nWex S Data\")\n",
    "wexs_sample_classes, wexs_sample_array, wexs_test_array, wexs_nwaves_array =  prep_for_norm(\n",
    "    wexs_samples_equal, wexs_test_data, wexs_not_waves, col_drop_normalisation\n",
    ")\n",
    "\n",
    "wexse_sample_classes, wexse_sample_array, wexse_test_array, wexs_nwaves_array =  prep_for_norm(\n",
    "    wexse_samples_equal, wexse_test_data, wexs_not_waves, col_drop_normalisation\n",
    ")\n",
    "\n",
    "wexsw_sample_classes, wexsw_sample_array, wexsw_test_array =  prep_for_norm(\n",
    "    wexsw_samples_equal, wexsw_test_data, pd.DataFrame(), col_drop_normalisation\n",
    ")\n",
    "\n",
    "print(\"\\nWex N Data\")\n",
    "wexn_sample_classes, wexn_sample_array, wexn_test_array, wexn_nwaves_array =  prep_for_norm(\n",
    "    wexn_samples_equal, wexn_test_data, wexn_not_waves, col_drop_normalisation\n",
    ")\n",
    "\n",
    "wexn_sample_scaled, wexn_test_scaled, wexn_nwaves_scaled = scaled_valid_deploy(\n",
    "     scaler, wexn_sample_array, wexn_test_array, wexn_nwaves_array\n",
    ")\n",
    "\n",
    "wexs_sample_scaled, wexs_test_scaled, wexs_nwaves_scaled = scaled_valid_deploy(\n",
    "     scaler, wexs_sample_array, wexs_test_array, wexs_nwaves_array\n",
    ")\n",
    "\n",
    "wexsw_sample_scaled, wexsw_test_scaled, wexsw_nwaves_scaled = scaled_valid_deploy(\n",
    "     scaler, wexsw_sample_array, wexsw_test_array, wexs_nwaves_array\n",
    ")\n",
    "\n",
    "wexse_sample_scaled, wexse_test_scaled, wexs_nwaves_scaled = scaled_valid_deploy(\n",
    "     scaler, wexse_sample_array, wexse_test_array, wexs_nwaves_array\n",
    ")\n",
    "\n",
    "wexto_sample_scaled = np.concatenate((\n",
    "    wexs_sample_scaled ,wexn_sample_scaled\n",
    "))\n",
    "\n",
    "wexto_test_scaled = np.concatenate((\n",
    "    wexs_test_scaled, wexn_test_scaled\n",
    "))\n",
    "\n",
    "wexto_train_model_dict[neulabel], wexto_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, wexto_sample_scaled, wexto_test_scaled\n",
    ")\n",
    "\n",
    "wexto_train_model_dict[neulabel2], wexto_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, wexto_sample_scaled, wexto_test_scaled\n",
    ")\n",
    "\n",
    "wexto_train_model_dict[voting], wexto_test_model_dict[voting] = model_deploy(\n",
    "    clf, wexto_sample_scaled, wexto_test_scaled\n",
    ")\n",
    "\n",
    "wexto_train_model_dict[svmlabel], wexto_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, wexto_sample_scaled, wexto_test_scaled\n",
    ")\n",
    "\n",
    "wexn_train_model_dict[neulabel], wexn_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, wexn_sample_scaled, wexn_test_scaled\n",
    ")\n",
    "\n",
    "wexn_train_model_dict[neulabel2], wexn_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, wexn_sample_scaled, wexn_test_scaled\n",
    ")\n",
    "\n",
    "wexn_train_model_dict[voting], wexn_test_model_dict[voting] = model_deploy(\n",
    "    clf, wexn_sample_scaled, wexn_test_scaled\n",
    ")\n",
    "\n",
    "wexn_train_model_dict[svmlabel], wexn_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, wexn_sample_scaled, wexn_test_scaled\n",
    ")\n",
    "\n",
    "wexs_train_model_dict[neulabel], wexs_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, wexs_sample_scaled, wexs_test_scaled\n",
    ")\n",
    "\n",
    "wexs_train_model_dict[neulabel2], wexs_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, wexs_sample_scaled, wexs_test_scaled\n",
    ")\n",
    "\n",
    "wexs_train_model_dict[voting], wexs_test_model_dict[voting] = model_deploy(\n",
    "    clf, wexs_sample_scaled, wexs_test_scaled\n",
    ")\n",
    "\n",
    "wexs_train_model_dict[svmlabel], wexs_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, wexs_sample_scaled, wexs_test_scaled\n",
    ")\n",
    "\n",
    "wexse_train_model_dict[neulabel], wexse_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, wexse_sample_scaled, wexse_test_scaled\n",
    ")\n",
    "\n",
    "wexse_train_model_dict[neulabel2], wexse_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, wexse_sample_scaled, wexse_test_scaled\n",
    ")\n",
    "\n",
    "wexse_train_model_dict[voting], wexse_test_model_dict[voting] = model_deploy(\n",
    "    clf, wexse_sample_scaled, wexse_test_scaled\n",
    ")\n",
    "\n",
    "wexse_train_model_dict[svmlabel], wexse_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, wexse_sample_scaled, wexse_test_scaled\n",
    ")\n",
    "\n",
    "wexsw_train_model_dict[neulabel], wexsw_test_model_dict[neulabel] = model_deploy(\n",
    "    neunet_model, wexsw_sample_scaled, wexsw_test_scaled\n",
    ")\n",
    "\n",
    "wexsw_train_model_dict[neulabel2], wexsw_test_model_dict[neulabel2] = model_deploy(\n",
    "    neunet_model2, wexsw_sample_scaled, wexsw_test_scaled\n",
    ")\n",
    "\n",
    "wexsw_train_model_dict[voting], wexsw_test_model_dict[voting] = model_deploy(\n",
    "    clf, wexsw_sample_scaled, wexsw_test_scaled\n",
    ")\n",
    "\n",
    "wexsw_train_model_dict[svmlabel], wexsw_test_model_dict[svmlabel] = model_deploy(\n",
    "    svm_model, wexsw_sample_scaled, wexsw_test_scaled\n",
    ")\n",
    "\n",
    "wexto_train_model_dict[voting] = np.concatenate((\n",
    "    wexs_train_model_dict[voting], wexn_train_model_dict[voting]\n",
    "))\n",
    "\n",
    "wexto_sample_classes = np.concatenate((\n",
    "    wexs_sample_classes, wexn_sample_classes\n",
    "))\n",
    "\n",
    "wexs_not_waves_dict[neulabel] = \"Flat\"\n",
    "wexn_not_waves_dict[neulabel] = \"Flat\"\n",
    "hem_not_waves_dict[neulabel] = \"Flat\"\n",
    "\n",
    "wexn_not_waves_dict[neulabel2] = \"Flat\"\n",
    "hem_not_waves_dict[neulabel2] = \"Flat\"\n",
    "wexs_not_waves_dict[neulabel2] = \"Flat\"\n",
    "\n",
    "wexn_not_waves_dict[svmlabel] = \"Flat\"\n",
    "hem_not_waves_dict[svmlabel] = \"Flat\"\n",
    "wexs_not_waves_dict[svmlabel] = \"Flat\"\n",
    "\n",
    "wexn_not_waves_dict[voting] = \"Flat\"\n",
    "hem_not_waves_dict[voting] = \"Flat\"\n",
    "wexs_not_waves_dict[voting] = \"Flat\"\n",
    "\n",
    "model = svmlabel\n",
    "\n",
    "class_labels = [\"Upper Slope\", \"Upper Stoss\", \"Lower Slope\", \"Lower Stoss\"]\n",
    "\n",
    "change_dict = {}\n",
    "\n",
    "wexse_change_dict = {}\n",
    "\n",
    "print(\"\\nCork \" + model + \" Model Train Score\")\n",
    "accuracy_analysis(\n",
    "    sample_classes, cork_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(cork_train_model_dict[model])]), \n",
    "    corkcm_image\n",
    ")\n",
    "\n",
    "print(\"\\nWex North \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    wexn_sample_classes, wexn_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(wexn_sample_classes)]), \n",
    "    wexncm_image\n",
    ")\n",
    "\n",
    "print(\"\\nWex South \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    wexs_sample_classes, wexs_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(wexs_sample_classes)]), \n",
    "    wexscm_image\n",
    ")\n",
    "\n",
    "print(\"\\nWex South West \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    wexse_sample_classes, wexse_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(wexse_sample_classes)])\n",
    ")\n",
    "\n",
    "print(\"\\nWex South East \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    wexsw_sample_classes, wexsw_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(wexsw_sample_classes)])\n",
    ")\n",
    "\n",
    "print(\"\\nWex Total \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    wexto_sample_classes, wexto_train_model_dict[model], model, \n",
    "    sorted([cla for cla in set(wexto_sample_classes)]), \n",
    "    wextocm_image\n",
    ")\n",
    "\n",
    "print(\"\\nHem \" + model + \" Model Valid Score\")\n",
    "accuracy_analysis(\n",
    "    hem_sample_classes, hem_train_model_dict[model], model,\n",
    "    sorted([cla for cla in set(hem_sample_classes)]),\n",
    "    hemcm_image\n",
    ")\n",
    "\n",
    "wexhem_classes = np.concatenate((wexto_sample_classes, hem_sample_classes))\n",
    "\n",
    "wexhem_pred = np.concatenate((wexto_train_model_dict[model], hem_train_model_dict[model]))\n",
    "\n",
    "print(\"\\nWexhem \" + model + \" Model Valid Score\")\n",
    "\n",
    "accuracy_analysis(\n",
    "    wexhem_classes, wexhem_pred, model, \n",
    "    sorted([cla for cla in set(wexhem_classes)]), wexhemcm_image\n",
    ")\n",
    "\n",
    "cork_sample_df = create_df(\n",
    "    samples_equal, samples_equal, cork_train_model_dict, collist, n_classes,\n",
    "     index =  samples_equal.index\n",
    ")\n",
    "\n",
    "cork_test_df = create_df(\n",
    "    test_data, test_data, cork_test_model_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "cork_not_waves_df = create_df(\n",
    "    not_waves, not_waves, cork_not_waves_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "wexs_sample_df = create_df(\n",
    "    wexs_samples_equal, wexs_samples_equal, wexs_train_model_dict, collist, n_classes,\n",
    "    index = wexs_samples_equal.index\n",
    ")\n",
    "\n",
    "wexn_sample_df = create_df(\n",
    "    wexn_samples_equal, wexn_samples_equal, wexn_train_model_dict, collist, n_classes,\n",
    "    index = wexn_samples_equal.index\n",
    ")\n",
    "\n",
    "wexs_test_df = create_df(\n",
    "    wexs_test_data, wexs_test_data, wexs_test_model_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "wexn_test_df = create_df(\n",
    "    wexn_test_data, wexn_test_data, wexn_test_model_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "wexs_not_waves_df = create_df(\n",
    "    wexs_not_waves, wexs_not_waves, wexs_not_waves_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "wexn_not_waves_df = create_df(\n",
    "    wexn_not_waves, wexn_not_waves, wexn_not_waves_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "hem_sample_df = create_df(\n",
    "    hem_samples_equal, hem_samples_equal, hem_train_model_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "hem_test_df = create_df(\n",
    "    hem_test_data, hem_test_data, hem_test_model_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "hem_not_waves_df = create_df(\n",
    "    hem_not_waves, hem_not_waves, hem_not_waves_dict, collist, n_classes\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = end - start\n",
    "\n",
    "print(duration)\n",
    "\n",
    "wexn_not_waves_dict[neulabel] = \"Flat\"\n",
    "wexn_not_waves_dict[svmlabel] = \"Flat\"\n",
    "\n",
    "hem_not_waves_dict[neulabel] = \"Flat\"\n",
    "hem_not_waves_dict[svmlabel] = \"Flat\"\n",
    "\n",
    "cork_classification_final = pd.concat([cork_sample_df, cork_test_df, cork_not_waves_df])\n",
    "\n",
    "wexs_classification_final = pd.concat([wexs_sample_df, wexs_test_df, wexs_not_waves_df])\n",
    "\n",
    "wexn_classification_final = pd.concat([wexn_sample_df, wexn_test_df, wexn_not_waves_df])\n",
    "\n",
    "hem_classification_final = pd.concat([hem_sample_df, hem_test_df, hem_not_waves_df])\n",
    "\n",
    "cork_classification_final_gdf = gpd.GeoDataFrame(\n",
    "    cork_classification_final, geometry = \"geometry\"\n",
    ")\n",
    "\n",
    "cork_classification_final_gdf[\"Area\"] = cork_classification_final_gdf.area\n",
    "\n",
    "wexn_classification_final_gdf = gpd.GeoDataFrame(\n",
    "    wexn_classification_final, geometry = \"geometry\"\n",
    ")\n",
    "\n",
    "wexn_classification_final_gdf[\"Area\"] = wexn_classification_final_gdf.area*(1 * 10**-6)\n",
    "\n",
    "wexs_classification_final_gdf = gpd.GeoDataFrame(\n",
    "    wexs_classification_final, geometry = \"geometry\"\n",
    ")\n",
    "\n",
    "wexs_classification_final_gdf[\"Area\"] = wexs_classification_final_gdf.area*(1 * 10**-6)\n",
    "\n",
    "hem_classification_final_gdf = gpd.GeoDataFrame(\n",
    "    hem_classification_final, geometry = \"geometry\"\n",
    ")\n",
    "\n",
    "wexs_sample_df.to_csv(r\"D:/Documents/PhD/Writing&Learning/Paper 1/WexSSamples.csv\")\n",
    "\n",
    "wexs_sample_df_group =  wexs_sample_df.groupby(\"Labels\")\n",
    "\n",
    "mean_group = wexs_sample_df_group.mean()\n",
    "\n",
    "mean_group.to_csv(r\"D:/Documents/PhD/Writing&Learning/Paper 1/WexSSamplesmean.csv\")\n",
    "\n",
    "wexs_sample_gdf = gpd.GeoDataFrame(wexs_sample_df, geometry = \"geometry\")\n",
    "\n",
    "wexs_sample_gdf.to_file(r\"D:/Documents/PhD/Writing&Learning/Paper 1/WexSSamples.shp\")\n",
    "\n",
    "hem_classification_final_gdf[\"Area\"] = hem_classification_final_gdf.area*(1 * 10**-6)\n",
    "\n",
    "cork_classification_final_gdf.to_file(classes_path_new)\n",
    "\n",
    "wexn_classification_final_gdf.to_file(wexn_classes_path_new)\n",
    "\n",
    "wexs_classification_final_gdf.to_file(wexs_classes_path_new)\n",
    "\n",
    "hem_classification_final_gdf.to_file(hem_classes_path_new)\n",
    "\n",
    "create_id_file(\n",
    "    classes_path_new, wexs_classes_path_new, intersect_path, csv_path, cork_dict, \n",
    "    test_data, neunet_model, neunet_model2\n",
    ")\n",
    "\n",
    "create_id_file(\n",
    "    wexs_classes_path_new, classes_path_new, intersect_path, wex_classes_path, wexs_dict, \n",
    "    wexs_test_data, neunet_model, neunet_model2\n",
    ")\n",
    "\n",
    "create_id_file(\n",
    "    wexn_classes_path_new, classes_path_new, intersect_path, wex_classes_path, wexn_dict,\n",
    "    wexn_test_data, neunet_model, neunet_model2\n",
    ")\n",
    "\n",
    "create_id_file(\n",
    "    hem_classes_path_new, classes_path_new, intersect_path, wex_classes_path, hem_dict,\n",
    "    hem_test_data, neunet_model, neunet_model2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the cross validation workflow. Please note that the scaling operation must be\n",
    "redeployed due to the splitting of the sampled data to avoid inclusion of information \n",
    "from the test dataset. Any model that is used can be taken from variables created in the\n",
    "prior cell i.e. svm_model. Outputs will show total rows in the training dataset, and the \n",
    "mean accuracy metrics for the classifier. \n",
    "\"\"\"\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "stratcv = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = 42)\n",
    "wexs = \"WexS\"\n",
    "wexn = \"WexN\"\n",
    "wexto = \"WexTo\"\n",
    "hem = \"Hem\"\n",
    "corktrain = \"Cork_Train\"\n",
    "corktest = \"Cork_Test\"\n",
    "\n",
    "scores = {} \n",
    "\n",
    "model = svm_model\n",
    "\n",
    "params = svm_params\n",
    "\n",
    "model_name = type(model).__name__\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "split_data = sample_scaled #Train data should be all data prior to normalisation\n",
    "#i.e., samples_equal\n",
    "cork_data = samples_equal.drop(labels = col_drop_normalisation, axis = 1, errors = \"ignore\")\n",
    "wexn_data = wexn_samples_equal.drop(labels = col_drop_normalisation, axis = 1, errors = \"ignore\")\n",
    "wexs_data = wexs_samples_equal.drop(labels = col_drop_normalisation, axis = 1, errors = \"ignore\")\n",
    "wexto_data = pd.concat([wexs_data, wexn_data])\n",
    "hem_data = hem_samples_equal.drop(labels = col_drop_normalisation, axis = 1, errors = \"ignore\")\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "cross = 0\n",
    "validation = 0\n",
    "\n",
    "for train_index, test_index in stratcv.split(split_data, sample_classes):\n",
    "\n",
    "    train_subset = cork_data.iloc[train_index].copy()\n",
    "\n",
    "    test_subset = cork_data.iloc[test_index].copy()\n",
    "    \n",
    "    print(train_subset.shape)\n",
    "\n",
    "    train_subset.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, \n",
    "        limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    train_subset_array = train_subset.to_numpy()\n",
    "\n",
    "    scaler2.fit(train_subset_array)\n",
    "\n",
    "    train_subset_scaled = scaler2.transform(train_subset_array)\n",
    "\n",
    "    model.fit(train_subset_scaled, sample_classes[train_index])\n",
    "\n",
    "    y_train_pred = model.predict(train_subset_scaled)\n",
    "\n",
    "    training_accuracy = metrics.balanced_accuracy_score(\n",
    "        sample_classes[train_index], y_train_pred\n",
    "    )\n",
    "\n",
    "    training_kappa_score = metrics.cohen_kappa_score(\n",
    "        sample_classes[train_index], y_train_pred\n",
    "    )\n",
    "\n",
    "    scores[\"fold\"] = cross\n",
    "\n",
    "    scores[\"params\"] = params\n",
    "\n",
    "    scores[\"train_kappa_score\"] = round(training_kappa_score, 3)\n",
    "\n",
    "    scores[\"train_acc_score\"] = round(training_accuracy, 3)\n",
    "\n",
    "    test_subset.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, \n",
    "        limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    test_subset_array = test_subset.to_numpy()\n",
    "\n",
    "    test_subset_scaled = scaler2.transform(\n",
    "        test_subset_array\n",
    "    )\n",
    "\n",
    "    y_test_pred = model.predict(test_subset_scaled)\n",
    "\n",
    "    test_kappa_score = metrics.cohen_kappa_score(\n",
    "        sample_classes[test_index], y_test_pred\n",
    "    )\n",
    "\n",
    "    scores[\"test_kappa_score\"] = round(test_kappa_score, 3)\n",
    "\n",
    "    test_accuracy = metrics.balanced_accuracy_score(\n",
    "        sample_classes[test_index], y_test_pred\n",
    "    )\n",
    "\n",
    "    scores[\"test_acc_score\"] = round(test_accuracy, 3)\n",
    "\n",
    "    scores[\"kappa_dif\"] = round(\n",
    "        abs(training_kappa_score - test_kappa_score),3\n",
    "    )\n",
    "    scores[\"acc_diff\"] = round(abs(training_accuracy - test_accuracy), 3)\n",
    "\n",
    "    wexn_data_copy = wexn_data.copy()\n",
    "\n",
    "    wexn_data_copy.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    wexn_data_array = wexn_data_copy.to_numpy()\n",
    "\n",
    "    wexn_data_scaled = scaler2.transform(\n",
    "        wexn_data_array\n",
    "    )\n",
    "\n",
    "    wexn_pred = model.predict(\n",
    "        wexn_data_scaled\n",
    "    )\n",
    "\n",
    "    wexn_validation_accuracy = metrics.balanced_accuracy_score(\n",
    "        wexn_sample_classes, wexn_pred\n",
    "    )\n",
    "\n",
    "    wexn_validation_kappa_coefficient = metrics.cohen_kappa_score(\n",
    "        wexn_sample_classes, wexn_pred\n",
    "    )\n",
    "\n",
    "    wexs_data_copy = wexs_data.copy()\n",
    "\n",
    "    wexs_data_copy.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, \n",
    "        limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    wexs_data_array = wexs_data_copy.to_numpy()\n",
    "\n",
    "    wexs_data_scaled = scaler2.transform(wexs_data_array)\n",
    "\n",
    "    wexs_pred = model.predict(wexs_data_scaled)\n",
    "\n",
    "    wexs_validation_accuracy = metrics.balanced_accuracy_score(\n",
    "        wexs_sample_classes, wexs_pred\n",
    "    )\n",
    "\n",
    "    wexs_validation_kappa_coefficient = metrics.cohen_kappa_score(\n",
    "        wexs_sample_classes, wexs_pred\n",
    "    )\n",
    "\n",
    "    wexto_data.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, \n",
    "        limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    wexto_data_array = wexto_data.to_numpy()\n",
    "\n",
    "    wexto_data_scaled = scaler2.transform(\n",
    "        wexto_data_array\n",
    "    )\n",
    "\n",
    "    wexto_pred = model.predict(\n",
    "        wexto_data_scaled\n",
    "    )\n",
    "\n",
    "    wexto_validation_accuracy = metrics.balanced_accuracy_score(\n",
    "        wexto_sample_classes, wexto_pred\n",
    "    )\n",
    "\n",
    "    wexto_validation_kappa_coefficient = metrics.cohen_kappa_score(\n",
    "        wexto_sample_classes, wexto_pred\n",
    "    )\n",
    "\n",
    "    hem_data.interpolate(\n",
    "        method = interpol, order = 2, axis = 0, inplace = True, \n",
    "        limit_direction = \"both\"\n",
    "    )\n",
    "\n",
    "    hem_data_array = hem_data.to_numpy()\n",
    "\n",
    "    hem_data_scaled = scaler2.transform(\n",
    "        hem_data_array\n",
    "    )\n",
    "\n",
    "    hem_pred = model.predict(\n",
    "        hem_data_scaled\n",
    "    )\n",
    "\n",
    "    hem_validation_accuracy = metrics.balanced_accuracy_score(\n",
    "                                    hem_sample_classes, hem_pred\n",
    "    )\n",
    "\n",
    "    hem_validation_kappa_coefficient = metrics.cohen_kappa_score(\n",
    "                                    hem_sample_classes, hem_pred\n",
    "    )        \n",
    "\n",
    "    scores[\"wexn_kappa_score\"] = round(wexn_validation_kappa_coefficient, 3)\n",
    "\n",
    "    scores[\"wexn_acc_score\"] = round(wexn_validation_accuracy, 3)\n",
    "\n",
    "    scores[\"wexs_kappa_score\"] = round(wexs_validation_kappa_coefficient, 3)\n",
    "\n",
    "    scores[\"wexs_acc_score\"] = round(wexs_validation_accuracy, 3) \n",
    "\n",
    "    scores[\"wexto_acc_score\"] = round(wexto_validation_accuracy, 3)\n",
    "\n",
    "    scores[\"wexto_kappa_score\"] = round(wexto_validation_kappa_coefficient, 3)\n",
    "\n",
    "    scores[\"hem_kappa_score\"] = round(hem_validation_kappa_coefficient , 3)\n",
    "\n",
    "    scores[\"hem_acc_score\"] = round(hem_validation_accuracy, 3)\n",
    "    \n",
    "    results_df = results_df.append(scores, ignore_index = True)\n",
    "    \n",
    "    cross += 1\n",
    "    \n",
    "print(\n",
    "    \"\\nCork Train Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df[\"train_kappa_score\"].mean(),\n",
    "    results_df[\"train_kappa_score\"].std())\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Cork Train Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.train_acc_score.mean(),\n",
    "     results_df.train_acc_score.std())\n",
    ")\n",
    "\n",
    "print(results_df.train_kappa_score.values)\n",
    "print(results_df.train_acc_score.values)\n",
    "\n",
    "print(\n",
    "    \"\\nCork Test Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.test_kappa_score.mean(),\n",
    "     results_df.test_kappa_score.std())\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Cork Test Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.test_acc_score.mean(),\n",
    "     results_df.test_acc_score.std())\n",
    ")\n",
    "\n",
    "print(results_df.test_kappa_score.values)\n",
    "print(results_df.test_acc_score.values)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nCork Diff Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.kappa_dif.mean(),\n",
    "     results_df.kappa_dif.std())\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Cork Diff Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.acc_diff.mean(),\n",
    "     results_df.acc_diff.std())\n",
    ")\n",
    "\n",
    "print(results_df.kappa_dif.values)\n",
    "print(results_df.acc_diff.values)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nWex North Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexn_kappa_score.mean(),\n",
    "     results_df.wexn_kappa_score.std())\n",
    ")\n",
    "print(\n",
    "    \"Wex North Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexn_acc_score.mean(),\n",
    "     results_df.wexn_acc_score.std())\n",
    ")\n",
    "\n",
    "print(results_df.wexn_kappa_score.values)\n",
    "print(results_df.wexn_acc_score.values)\n",
    "\n",
    "print(\n",
    "    \"\\nWex South Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexs_kappa_score.mean(),\n",
    "     results_df.wexs_kappa_score.std())\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Wex South Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexs_acc_score.mean(),\n",
    "     results_df.wexs_acc_score.std())\n",
    ")\n",
    "\n",
    "print(results_df.wexs_kappa_score.values)\n",
    "print(results_df.wexs_acc_score.values)\n",
    "\n",
    "print(\n",
    "    \"\\nWex Total Kappas: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexto_kappa_score.mean(),\n",
    "     results_df.wexto_kappa_score.std())\n",
    ")\n",
    "print(\n",
    "    \"Wex Total Accuracies: %0.4f (+/- %0.4f)\" % \\\n",
    "    (results_df.wexto_acc_score.mean(),\n",
    "     results_df.wexto_acc_score.std())\n",
    ")\n",
    "\n",
    "print(results_df.wexto_kappa_score.values)\n",
    "print(results_df.wexto_acc_score.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell will seek to save any cross validation results. \n",
    "\"\"\"\n",
    "print(model_name)\n",
    "cv_path_root = r\"D:/Documents/PhD/Writing&Learning/Paper 1/Cross_Validation\"\n",
    "cv_path = os.path.join(\n",
    "    cv_path_root, str(model_name) + str(iteration) + \"2.csv\"\n",
    ").replace(\"\\\\\", \"/\")\n",
    "\n",
    "cols = [\n",
    "    \"Train Accuracy\", \"Test Accuracy\", \"Acc Diff\", \"Train Kappa\", \"Test Kappa\",\n",
    "    \"Kappa Diff\"\n",
    "]\n",
    "\n",
    "train_kappa = results_df.train_kappa_score.to_list()\n",
    "train_kappa = [i * 100 for i in train_kappa]\n",
    "train_accuracy = results_df.train_acc_score.to_list()\n",
    "train_accuracy = [i * 100 for i in train_accuracy]\n",
    "test_kappa = results_df.test_kappa_score.to_list()\n",
    "test_kappa = [i * 100 for i in test_kappa]\n",
    "test_accuracy = results_df.test_acc_score.to_list()\n",
    "test_accuracy = [i * 100 for i in test_accuracy]\n",
    "train_test_acc_diff = results_df.acc_diff.to_list()\n",
    "train_test_acc_diff =  [i * 100 for i in train_test_acc_diff]\n",
    "train_test_kappa_diff = results_df.kappa_dif.to_list()\n",
    "train_test_kappa_diff = [i * 100 for i in train_test_kappa_diff]\n",
    "\n",
    "accuracy_list = list(\n",
    "    zip(\n",
    "    train_accuracy, test_accuracy, train_test_acc_diff, train_kappa, test_kappa,\n",
    "    train_test_kappa_diff\n",
    "    )\n",
    ")\n",
    "\n",
    "dataframe = pd.DataFrame(accuracy_list, columns = cols)\n",
    "\n",
    "mean = [\n",
    "    st.mean(train_accuracy),st.mean(test_accuracy), st.mean(train_test_acc_diff), \n",
    "    st.mean(train_kappa), st.mean(test_kappa), st.mean(train_test_kappa_diff)\n",
    "]\n",
    "\n",
    "std = [\n",
    "    round(st.stdev(train_accuracy),2), round(st.stdev(test_accuracy),2), \n",
    "    round(st.stdev(train_test_acc_diff), 2), round(st.stdev(train_kappa), 2),\n",
    "    round(st.stdev(test_kappa), 2), round(st.stdev(train_test_kappa_diff), 2)   \n",
    "    \n",
    "]\n",
    "folds = [*range(1,11)]\n",
    "\n",
    "print(len(mean))\n",
    "\n",
    "folds.extend([\"Mean\", \"Standard Deviation\"])\n",
    "\n",
    "print(folds)\n",
    "\n",
    "mean_len = dataframe.shape[0]\n",
    "\n",
    "dataframe.loc[mean_len] = mean\n",
    "\n",
    "print(dataframe.head(11))\n",
    "\n",
    "std_len = dataframe.shape[0]\n",
    "\n",
    "dataframe.loc[std_len] = std\n",
    "\n",
    "print(dataframe.head(13))\n",
    "\n",
    "dataframe.insert(loc = 0, column = \"Fold\", value = folds)\n",
    "\n",
    "print(cv_path)\n",
    "\n",
    "dataframe.to_csv(cv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code creates the parameter grid for the hyperparameter tuning, this can be altered\n",
    "to suit each individual classifier algorithm. \n",
    "\"\"\"\n",
    "rsrange = range(1, 11)\n",
    "\n",
    "linearsvmparams = {\n",
    "    \"penalty\": [\"l1\", \"l2\"], \"loss\": [\"hinge\", \"squared_hinge\"], \"dual\": [True, False],\n",
    "    \"tol\": [0.001, 0.01, 0.1, 1], \n",
    "    \"C\": [1, 10, 100, 1000, 0.1, 0.01], \"multi_class\": [\"ovr\", \"crammer_singer\"],\n",
    "    \"fit_intercept\": [True, False], \"random_state\": [i for i in rsrange]\n",
    "}\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "start_cv = time.time()\n",
    "\n",
    "paramgrid = list(ParameterGrid(linearsvmparams))\n",
    "\n",
    "data = samples_equal.copy()\n",
    "\n",
    "print([i for i in rsrange])\n",
    "print(len(paramgrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains the code required to distribute the hyperparameter tuning process across\n",
    "multiple cores. We recommend that this is conducted in runs of 10000 parameter sets or less as \n",
    "a memory leak in the code prevents full utilisation of the code across larger sets of parameters.\n",
    "The kappa coefficient is the metric that we chose to evaluate classification efficacy, other\n",
    "parameters can be input to replace this. \n",
    "\"\"\"\n",
    "\n",
    "import ray\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical = False) - 1\n",
    "\n",
    "start_ray = time.time()\n",
    "\n",
    "better_scores = []\n",
    "\n",
    "best_score = 0.525\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ray.init(num_cpus = num_cpus) \n",
    "    \n",
    "    @ray.remote\n",
    "    def parallel_processing(\n",
    "        model, pm, train_data, train_labels, wexto_test_data, wexto_test_labels, scoredict\n",
    "    ):\n",
    "        model.set_params(**pm)\n",
    "        try:\n",
    "            model.fit(train_data, train_labels)\n",
    "            wexto_test_pred = model.predict(wexto_test_data)\n",
    "            tokappa_score = metrics.cohen_kappa_score(wexto_test_labels, wexto_test_pred)\n",
    "            if tokappa_score >= best_score:\n",
    "                print(\"\\nPay dirt: \" + str(round(tokappa_score, 3)))\n",
    "                best_data = pm\n",
    "                return (tokappa_score, best_data)\n",
    "            else:\n",
    "                print(\"\\nNo: \" + str(round(tokappa_score, 3)))\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    results = ray.get([\n",
    "        parallel_processing.remote(\n",
    "            svm, parameter, sample_scaled, sample_classes, \n",
    "            wexto_sample_scaled_equal, wexto_sample_classes_equal, better_scores\n",
    "        ) for parameter in paramgrid\n",
    "    ])\n",
    "    \n",
    "ray.shutdown()\n",
    "end_ray = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell allows for the shutdown of the ray kernel and display of the resultant\n",
    "parameters. All null values are filtered from the list at this time.\n",
    "\"\"\"\n",
    "ray.shutdown()\n",
    "end_ray = time.time()\n",
    "totaltime = (end_ray - start_ray)/60\n",
    "resultsreal = list(filter(None, results))\n",
    "print(totaltime)\n",
    "print(resultsreal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code here is written to create combinations of MLP layers. This code is designed to extract\n",
    "the combination with the highest accuracy score for the low resolution dataset. \n",
    "\"\"\"\n",
    "better_kappa = 0\n",
    "for combo in combinations(param_list, 2):\n",
    "    estimators = [\n",
    "        (\"mlp1\", MLPClassifier(**combo[0])),\n",
    "        (\"mlp2\", MLPClassifier(**combo[1]))\n",
    "    ] \n",
    "    for com in combinations([1,1.5, 2, 1.25, 1], 2):\n",
    "        weights = [com[0], com[1]]\n",
    "        model = VotingClassifier(estimators, voting = \"soft\", weights = weights)\n",
    "        model.fit(sample_scaled, sample_classes)\n",
    "        testo_pred = model.predict(wexto_sample_scaled)\n",
    "        testo_accuracy = metrics.balanced_accuracy_score(wexto_sample_classes, testo_pred) \n",
    "        testo_kappa = metrics.cohen_kappa_score(wexto_sample_classes, testo_pred) \n",
    "        if testo_kappa > better_kappa:\n",
    "            better_kappa = testo_kappa\n",
    "            print(estimators)\n",
    "            print(weights)\n",
    "            print(testo_kappa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
